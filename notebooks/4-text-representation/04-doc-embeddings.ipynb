{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SAE-78: Document Embeddings\n",
                "\n",
                "Ce notebook implémente la création de vecteurs de documents (reviews) à partir du modèle Word2Vec (SAE-77)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# Ajouter le dossier src au path (niveau supérieur car on est dans notebooks/4-text-representation/)\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
                "\n",
                "from src.features import compute_doc_embeddings\n",
                "from src.text_preprocessing import preprocess_text"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Chargement des données"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Charger les reviews nettoyées\n",
                "DATA_PATH = '../../data/cleaned/reviews_clean.parquet'\n",
                "\n",
                "try:\n",
                "    df = pd.read_parquet(DATA_PATH)\n",
                "    print(f\"Loaded {len(df)} reviews.\")\n",
                "    \n",
                "    # POUR LE DEV/DEMO : On prend un échantillon si le dataset est trop gros\n",
                "    # df = df.head(10000).copy()\n",
                "    \n",
                "except FileNotFoundError:\n",
                "    print(f\"File not found: {DATA_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Préparation (Tokenisation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# On s'assure d'avoir une colonne de tokens\n",
                "if 'text_preprocessed' not in df.columns:\n",
                "    print(\"Preprocessing text...\")\n",
                "    # Attention: sur tout le dataset, ça peut être long.\n",
                "    # Idéalement cette étape devrait être faite en amont et sauvegardée.\n",
                "    df['text_preprocessed'] = df['text'].apply(lambda x: preprocess_text(str(x)))\n",
                "\n",
                "# Tokenisation simple (split) car text_preprocessed est déjà nettoyé (espaces)\n",
                "print(\"Tokenizing...\")\n",
                "df['tokens'] = df['text_preprocessed'].apply(lambda x: x.split() if isinstance(x, str) else [])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Calcul des Embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_PATH = '../../outputs/models/word2vec_yelp.model'\n",
                "\n",
                "print(\"Computing document embeddings...\")\n",
                "embeddings = compute_doc_embeddings(df['tokens'], model_path=MODEL_PATH)\n",
                "\n",
                "print(f\"Embeddings shape: {embeddings.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Sauvegarde des résultats\n",
                "On sauvegarde les embeddings pour pouvoir les réutiliser (Clustering, Classif...)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Créer un DataFrame avec IDs et Embeddings\n",
                "# On ne peut pas facilement sauvegarder un array numpy dans parquet directement sans serialisation,\n",
                "# ou alors on garde 1 colonne par dimension (ex: emb_0, emb_1...).\n",
                "# Pour simplifier, on stocke souvent sous forme de liste dans parquet.\n",
                "\n",
                "df_emb = df[['review_id']].copy()\n",
                "df_emb['doc_embedding'] = list(embeddings)\n",
                "\n",
                "OUTPUT_PATH = '../../data/cleaned/reviews_embeddings.parquet'\n",
                "\n",
                "df_emb.to_parquet(OUTPUT_PATH)\n",
                "print(f\"Embeddings saved to {OUTPUT_PATH}\")\n",
                "print(df_emb.head())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}