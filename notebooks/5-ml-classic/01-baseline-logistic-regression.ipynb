{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SAE-81 \u2013 Classification Baseline \u2013 Logistic Regression\n",
                "\n",
                "**En tant que** ML engineer  \n",
                "**Je veux** un mod\u00e8le baseline de classification  \n",
                "**Afin de** avoir un point de r\u00e9f\u00e9rence simple\n",
                "\n",
                "## Objectifs\n",
                "\n",
                "- Split train/test (80/20, stratifi\u00e9)\n",
                "- Logistic Regression avec TF-IDF (de SAE-76)\n",
                "- M\u00e9triques : Accuracy, Precision, Recall, F1\n",
                "- Classification report\n",
                "- Sauvegarder le mod\u00e8le\n",
                "\n",
                "## Input / Output\n",
                "\n",
                "- **Input** : `data/cleaned/reviews_text_cleaned.parquet` + `outputs/models/tfidf_vectorizer.pkl`\n",
                "- **Output** : `outputs/models/lr_baseline.pkl`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pickle\n",
                "from pathlib import Path\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import (\n",
                "    classification_report,\n",
                "    accuracy_score,\n",
                "    precision_score,\n",
                "    recall_score,\n",
                "    f1_score\n",
                ")\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"Imports OK\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Chargement des Donn\u00e9es et du Vectorizer TF-IDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration des chemins\n",
                "DATA_PATH = Path('../../data/cleaned/reviews_text_cleaned.parquet')\n",
                "TFIDF_MODEL_PATH = Path('../../outputs/models/tfidf_vectorizer.pkl')\n",
                "LR_MODEL_PATH = Path('../../outputs/models/lr_baseline.pkl')\n",
                "\n",
                "# Creer le dossier output si necessaire\n",
                "LR_MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"Data: {DATA_PATH}\")\n",
                "print(f\"TF-IDF model: {TFIDF_MODEL_PATH}\")\n",
                "print(f\"LR output: {LR_MODEL_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chargement des donnees\n",
                "df = pd.read_parquet(DATA_PATH)\n",
                "print(f\"Donnees chargees: {len(df):,} reviews\")\n",
                "print(f\"Colonnes: {list(df.columns)}\")\n",
                "\n",
                "# Verification des colonnes necessaires\n",
                "assert 'text_cleaned' in df.columns, \"Colonne 'text_cleaned' manquante!\"\n",
                "assert 'stars' in df.columns, \"Colonne 'stars' manquante!\"\n",
                "\n",
                "# Remplir les NaNs\n",
                "df['text_cleaned'] = df['text_cleaned'].fillna('')\n",
                "\n",
                "# Apercu\n",
                "print(f\"\\nDistribution des etoiles:\")\n",
                "print(df['stars'].value_counts().sort_index())\n",
                "df[['text_cleaned', 'stars']].head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chargement du TF-IDF vectorizer pre-entraine (SAE-76)\n",
                "with open(TFIDF_MODEL_PATH, 'rb') as f:\n",
                "    tfidf_vectorizer = pickle.load(f)\n",
                "\n",
                "print(f\"TF-IDF vectorizer charge\")\n",
                "print(f\"  Type: {type(tfidf_vectorizer).__name__}\")\n",
                "print(f\"  Params: ngram_range={tfidf_vectorizer.ngram_range}, max_features={tfidf_vectorizer.max_features}\")\n",
                "print(f\"  Vocabulaire: {len(tfidf_vectorizer.vocabulary_):,} termes\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Pr\u00e9paration des Features TF-IDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transformer les textes en features TF-IDF\n",
                "X = tfidf_vectorizer.transform(df['text_cleaned'])\n",
                "y = df['stars']\n",
                "\n",
                "print(f\"Features TF-IDF generees\")\n",
                "print(f\"  X shape: {X.shape}\")\n",
                "print(f\"  y shape: {y.shape}\")\n",
                "print(f\"  Sparsity: {(1 - X.nnz / (X.shape[0] * X.shape[1])) * 100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Split Train / Test (80/20)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split stratifie\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y,\n",
                "    test_size=0.2,\n",
                "    random_state=42,\n",
                "    stratify=y\n",
                ")\n",
                "\n",
                "print(f\"Split train/test effectue\")\n",
                "print(f\"  Train: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(df)*100:.0f}%)\")\n",
                "print(f\"  Test:  {X_test.shape[0]:,} samples ({X_test.shape[0]/len(df)*100:.0f}%)\")\n",
                "print(f\"\\nDistribution train:\")\n",
                "print(y_train.value_counts().sort_index())\n",
                "print(f\"\\nDistribution test:\")\n",
                "print(y_test.value_counts().sort_index())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Entra\u00eenement \u2013 Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Entrainement du modele\n",
                "import time\n",
                "\n",
                "print(\"Entrainement de la Logistic Regression...\")\n",
                "start = time.time()\n",
                "\n",
                "lr = LogisticRegression(\n",
                "    max_iter=1000,\n",
                "    random_state=42,\n",
                "    solver='lbfgs'\n",
                ")\n",
                "lr.fit(X_train, y_train)\n",
                "\n",
                "duration = time.time() - start\n",
                "print(f\"Modele entraine en {duration:.2f}s\")\n",
                "print(f\"  Classes: {lr.classes_}\")\n",
                "print(f\"  Nombre d'iterations: {lr.n_iter_}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Pr\u00e9dictions et \u00c9valuation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predictions\n",
                "y_pred = lr.predict(X_test)\n",
                "\n",
                "# Metriques globales\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "precision = precision_score(y_test, y_pred, average='weighted')\n",
                "recall = recall_score(y_test, y_pred, average='weighted')\n",
                "f1 = f1_score(y_test, y_pred, average='weighted')\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"METRIQUES GLOBALES\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
                "print(f\"  Precision: {precision:.4f} (weighted)\")\n",
                "print(f\"  Recall:    {recall:.4f} (weighted)\")\n",
                "print(f\"  F1-score:  {f1:.4f} (weighted)\")\n",
                "print(\"=\" * 50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification Report detaille\n",
                "print(\"\\nCLASSIFICATION REPORT\")\n",
                "print(\"=\" * 60)\n",
                "report = classification_report(y_test, y_pred)\n",
                "print(report)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification report sous forme de DataFrame pour analyse\n",
                "report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
                "report_df = pd.DataFrame(report_dict).T\n",
                "print(\"Metriques par classe:\")\n",
                "display(report_df.style.format('{:.3f}').background_gradient(cmap='YlGn', subset=['f1-score']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Sauvegarde du Mod\u00e8le"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sauvegarde du modele\n",
                "with open(LR_MODEL_PATH, 'wb') as f:\n",
                "    pickle.dump(lr, f)\n",
                "\n",
                "print(f\"Modele sauvegarde: {LR_MODEL_PATH}\")\n",
                "print(f\"  Taille: {LR_MODEL_PATH.stat().st_size / 1024:.1f} KB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verification du rechargement\n",
                "with open(LR_MODEL_PATH, 'rb') as f:\n",
                "    loaded_lr = pickle.load(f)\n",
                "\n",
                "# Test de prediction avec le modele recharge\n",
                "y_pred_check = loaded_lr.predict(X_test[:5])\n",
                "print(f\"Modele recharge et fonctionnel\")\n",
                "print(f\"  Predictions test: {y_pred_check}\")\n",
                "print(f\"  Vraies valeurs:   {y_test.iloc[:5].values}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. R\u00e9sum\u00e9\n",
                "\n",
                "### R\u00e9sultats du Baseline\n",
                "\n",
                "| M\u00e9trique | Valeur |\n",
                "|----------|--------|\n",
                "| Accuracy | cf. ci-dessus |\n",
                "| Precision (weighted) | cf. ci-dessus |\n",
                "| Recall (weighted) | cf. ci-dessus |\n",
                "| F1-score (weighted) | cf. ci-dessus |\n",
                "\n",
                "### Points cl\u00e9s\n",
                "\n",
                "- **Mod\u00e8le** : Logistic Regression (solver=lbfgs, max_iter=1000)\n",
                "- **Features** : TF-IDF Bigrams (5000 features, SAE-76)\n",
                "- **Split** : 80% train / 20% test (stratifi\u00e9, random_state=42)\n",
                "- **Sauvegarde** : `outputs/models/lr_baseline.pkl`\n",
                "\n",
                "Ce mod\u00e8le servira de **baseline** pour comparer avec des mod\u00e8les plus avanc\u00e9s (SVM, Random Forest, BERT, etc.)."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}