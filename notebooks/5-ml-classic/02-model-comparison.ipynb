{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SAE-82 \u2013 Comparaison Mod\u00e8les ML (SVM, RF, NB)\n",
                "\n",
                "**En tant que** ML engineer  \n",
                "**Je veux** tester plusieurs mod\u00e8les ML  \n",
                "**Afin de** trouver le meilleur pour la classification\n",
                "\n",
                "## Objectifs\n",
                "\n",
                "- Entra\u00eener 4 mod\u00e8les : Logistic Regression, LinearSVC, Random Forest, Naive Bayes\n",
                "- Comparer les performances (Accuracy, F1, Precision, Recall, temps)\n",
                "- Sauvegarder le meilleur mod\u00e8le\n",
                "\n",
                "## Input / Output\n",
                "\n",
                "- **Input** : `data/cleaned/reviews_text_cleaned.parquet` + `outputs/models/tfidf_vectorizer.pkl`\n",
                "- **Output** : `outputs/ml_models_comparison.csv` + meilleur mod\u00e8le `.pkl`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pickle\n",
                "import time\n",
                "from pathlib import Path\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import LinearSVC\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.metrics import (\n",
                "    classification_report,\n",
                "    accuracy_score,\n",
                "    precision_score,\n",
                "    recall_score,\n",
                "    f1_score\n",
                ")\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"Imports OK\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Chargement et Pr\u00e9paration des Donn\u00e9es"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chemins\n",
                "DATA_PATH = Path('../../data/cleaned/reviews_text_cleaned.parquet')\n",
                "TFIDF_MODEL_PATH = Path('../../outputs/models/tfidf_vectorizer.pkl')\n",
                "OUTPUT_CSV = Path('../../outputs/ml_models_comparison.csv')\n",
                "BEST_MODEL_PATH = Path('../../outputs/models/best_ml_model.pkl')\n",
                "\n",
                "# Charger donnees\n",
                "df = pd.read_parquet(DATA_PATH)\n",
                "df['text_cleaned'] = df['text_cleaned'].fillna('')\n",
                "print(f\"Donnees chargees: {len(df):,} reviews\")\n",
                "\n",
                "# Charger TF-IDF\n",
                "with open(TFIDF_MODEL_PATH, 'rb') as f:\n",
                "    tfidf = pickle.load(f)\n",
                "print(f\"TF-IDF charge: {len(tfidf.vocabulary_):,} termes\")\n",
                "\n",
                "# Transformer\n",
                "X = tfidf.transform(df['text_cleaned'])\n",
                "y = df['stars']\n",
                "print(f\"X shape: {X.shape}\")\n",
                "\n",
                "# Distribution\n",
                "print(f\"\\nDistribution des etoiles:\")\n",
                "print(y.value_counts().sort_index())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split train/test\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "print(f\"Train: {X_train.shape[0]:,} | Test: {X_test.shape[0]:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. D\u00e9finition des Mod\u00e8les"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "models = {\n",
                "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, solver='lbfgs'),\n",
                "    'LinearSVC (SVM)': LinearSVC(max_iter=1000, random_state=42),\n",
                "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
                "    'Naive Bayes': MultinomialNB()\n",
                "}\n",
                "\n",
                "print(f\"Modeles a entrainer: {len(models)}\")\n",
                "for name in models:\n",
                "    print(f\"  - {name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Entra\u00eenement et \u00c9valuation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "trained_models = {}\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"  {name}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    # Entrainement\n",
                "    start = time.time()\n",
                "    model.fit(X_train, y_train)\n",
                "    train_time = time.time() - start\n",
                "    \n",
                "    # Predictions\n",
                "    y_pred = model.predict(X_test)\n",
                "    \n",
                "    # Metriques\n",
                "    acc = accuracy_score(y_test, y_pred)\n",
                "    prec = precision_score(y_test, y_pred, average='weighted')\n",
                "    rec = recall_score(y_test, y_pred, average='weighted')\n",
                "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
                "    \n",
                "    results.append({\n",
                "        'Model': name,\n",
                "        'Accuracy': acc,\n",
                "        'Precision': prec,\n",
                "        'Recall': rec,\n",
                "        'F1': f1,\n",
                "        'Train Time (s)': round(train_time, 2)\n",
                "    })\n",
                "    \n",
                "    trained_models[name] = model\n",
                "    \n",
                "    print(f\"  Accuracy:  {acc:.4f}\")\n",
                "    print(f\"  Precision: {prec:.4f}\")\n",
                "    print(f\"  Recall:    {rec:.4f}\")\n",
                "    print(f\"  F1:        {f1:.4f}\")\n",
                "    print(f\"  Temps:     {train_time:.2f}s\")\n",
                "\n",
                "print(f\"\\n\\nTous les modeles entraines!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Tableau Comparatif"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Creer le tableau comparatif\n",
                "results_df = pd.DataFrame(results).sort_values('F1', ascending=False)\n",
                "results_df.index = range(1, len(results_df) + 1)\n",
                "results_df.index.name = 'Rank'\n",
                "\n",
                "print(\"TABLEAU COMPARATIF DES MODELES\")\n",
                "print(\"=\" * 80)\n",
                "display(results_df.style.format({\n",
                "    'Accuracy': '{:.4f}',\n",
                "    'Precision': '{:.4f}',\n",
                "    'Recall': '{:.4f}',\n",
                "    'F1': '{:.4f}',\n",
                "    'Train Time (s)': '{:.2f}'\n",
                "}).background_gradient(cmap='YlGn', subset=['F1', 'Accuracy']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sauvegarder CSV\n",
                "OUTPUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
                "results_df.to_csv(OUTPUT_CSV, index=False)\n",
                "print(f\"Tableau sauvegarde: {OUTPUT_CSV}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Classification Report D\u00e9taill\u00e9 par Mod\u00e8le"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for name, model in trained_models.items():\n",
                "    y_pred = model.predict(X_test)\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"  Classification Report: {name}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Sauvegarde du Meilleur Mod\u00e8le"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identifier le meilleur modele\n",
                "best_row = results_df.iloc[0]\n",
                "best_name = best_row['Model']\n",
                "best_model = trained_models[best_name]\n",
                "\n",
                "print(f\"Meilleur modele: {best_name}\")\n",
                "print(f\"  F1: {best_row['F1']:.4f}\")\n",
                "print(f\"  Accuracy: {best_row['Accuracy']:.4f}\")\n",
                "\n",
                "# Sauvegarder\n",
                "BEST_MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
                "with open(BEST_MODEL_PATH, 'wb') as f:\n",
                "    pickle.dump(best_model, f)\n",
                "\n",
                "print(f\"\\nModele sauvegarde: {BEST_MODEL_PATH}\")\n",
                "print(f\"  Taille: {BEST_MODEL_PATH.stat().st_size / 1024:.1f} KB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verification du rechargement\n",
                "with open(BEST_MODEL_PATH, 'rb') as f:\n",
                "    loaded = pickle.load(f)\n",
                "\n",
                "y_check = loaded.predict(X_test[:5])\n",
                "print(f\"Modele recharge OK\")\n",
                "print(f\"  Predictions: {y_check}\")\n",
                "print(f\"  Vraies:      {y_test.iloc[:5].values}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. R\u00e9sum\u00e9\n",
                "\n",
                "### Conclusions\n",
                "\n",
                "- Le tableau comparatif ci-dessus montre les performances de chaque mod\u00e8le\n",
                "- Le meilleur mod\u00e8le est sauvegard\u00e9 dans `outputs/models/best_ml_model.pkl`\n",
                "- Le tableau CSV est dans `outputs/ml_models_comparison.csv`\n",
                "- Les classes extr\u00eames (1\u2605 et 5\u2605) sont g\u00e9n\u00e9ralement mieux class\u00e9es que les interm\u00e9diaires\n",
                "\n",
                "### Mod\u00e8les test\u00e9s\n",
                "\n",
                "| Mod\u00e8le | Description |\n",
                "|--------|-------------|\n",
                "| Logistic Regression | Baseline (SAE-81) |\n",
                "| LinearSVC | SVM lin\u00e9aire optimis\u00e9 |\n",
                "| Random Forest | Ensemble de 100 arbres |\n",
                "| Naive Bayes | MultinomialNB |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}